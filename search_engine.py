# -*- coding: utf-8 -*-
"""Search engine.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fG4fUQbFwNBNhpkiB0RNy_9f1ipm4DbU
"""

!pip install rank_bm25

from rank_bm25 import BM25Okapi

corpus = [
    "Hello there good man!",
    "It is quite windy in London",
    "How is the weather today?"
]

tokenized_corpus = [doc.split(" ") for doc in corpus]

bm25 = BM25Okapi(tokenized_corpus)

query = "quite windy"
tokenized_query = query.split(" ")
bm25.get_top_n(tokenized_query, corpus, n=1)

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed May 26 10:28:33 2021

@author: lapis
"""
import numpy as np
from rank_bm25 import BM25Okapi
import urllib.request


lelien=["https://hal.archives-ouvertes.fr/search/index/?q=bourbaki&docType_s=OUV+OR+ART+OR+LECTURE+OR+THESE+OR+COMM+OR+COUV+OR+REPORT&rows=100"]

directory='/home/lapis/Desktop/searchengine/donneesrechercheMaths.txt' 

#liste de liens pour avoir des articles de tous les domaines
maliste=["https://hal.archives-ouvertes.fr/search/index/?q=%2A&level0_domain_s=phys&rows=100","https://hal.archives-ouvertes.fr/search/index/?q=%2A&level0_domain_s=shs&rows=100","https://hal.archives-ouvertes.fr/search/index/?q=%2A&rows=100&level0_domain_s=info","https://hal.archives-ouvertes.fr/search/index/?q=%2A&level0_domain_s=chim&rows=100","https://hal.archives-ouvertes.fr/search/index/?q=%2A&level0_domain_s=math&rows=100","https://hal.archives-ouvertes.fr/search/index/?q=%2A&level0_domain_s=sde&rows=100","https://hal.archives-ouvertes.fr/search/index/?q=%2A&level0_domain_s=stat&rows=100"]
#liste de liens pour avoir des articles de maths <3
maliste2=["https://hal.archives-ouvertes.fr/search/index/?q=%2A&level0_domain_s=math&rows=100"]    



"""example d'utilisation library bm25 responsable des requetes de recherche
corpus = [
    "Hello there quite quite good man!",
    "It is quite windy in London",
    "How is the weather today?"
]

tokenized_corpus = [doc.split(" ") for doc in corpus]

bm25 = BM25Okapi(tokenized_corpus)

query = "quite"
tokenized_query = query.split(" ")
doc_scores = bm25.get_scores(tokenized_query)
print(bm25.get_top_n(tokenized_query, corpus, n=3))"""

def normalisation(mot, accent=True):
    """
    Cette fonction normalise un mot (enlève la ponctuation à la fin, met tout en minuscule).
    Par défaut elle laisse les accents intacts mais elle peut les supprimer si demandé.
    """
    #traitement ponctuation
    ponctuation=[",",";",".","?","!",":",")"]
    for el in ponctuation:
        mot=mot.rstrip(el)
        
    #traitement majuscule
    L_majuscule=["A","B","C","D","E","F","G","H","I","J","K","L","M","N","O","P","Q","R","S","T","U","V","W","X","Y","Z"]
    dico_lettre={"A":"a","B":"b","C":"c","D":"d","E":"e","F":"f","G":"g","H":"h","I":"i","J":"j","K":"k","L":"l","M":"m","N":"n","O":"o","P":"p","Q":"q","R":"r","S":"s","T":"t","U":"u","V":"v","W":"w","X":"x","Y":"y","Z":"z"}
    
    for i in range(len(mot)):
        if mot[i] in L_majuscule:
            mot=mot[:i]+dico_lettre[mot[i]]+mot[i+1:] 
    
    #traitement accent
    if accent:
      
        dico_accent={"ê":"e","ë":"e","é":"e","è":"e","â":"a","ä":"a","à":"a","ü":"u","û":"u","ù":"u","ï":"i","î":"i","ö":"o","ô":"o","ÿ":"y","É":"e"}
        L_accent = dico_accent.keys()

        for i in range(len(mot)):
            if mot[i] in L_accent:
                mot=mot[:i]+dico_accent[mot[i]]+mot[i+1:]
    
    return mot #le return est necesssaire.


def recuphtml(link) : 
  #retourne le code source d'un URL
  # open a connection to a URL using urllib
  webUrl  = urllib.request.urlopen(link)
  # class="media-body"
  data = webUrl.read()
  return data

def recupid (data) :
  # retourne une liste d'ids d'articles à partir du code source d'une page de recherche Hal
  data=data.decode('utf-8')
  chaine = 'class="ref-halid">'
  lon = len(chaine)
  liste = data.split()
  results = []
  for element in liste :
    if chaine in element :
      ind = element.index('<')
      results.append(element[lon:ind]+element[ind+7:ind+9])
  return results



def getlink(ids):
  #retourne une liste de liens à telecharger pour récuperer les articles à partir d'une liste d'ids
  links = []
  for k in ids :
    links = links + ["https://hal.archives-ouvertes.fr/"+k+"/document"]
  return ids,links

def keywords(id):
    # renvoie les mots clés sous forme d'une chaine de caractères séparés par des espaces
    chaine='btn-xs">'
    chaine2 = '</a>'
    data = recuphtml("https://hal.archives-ouvertes.fr/"+id)
    data=data.decode('utf-8')
    liste=data.split()
    kw=[]
    for k in range(len(liste)):
        if chaine in liste[k]:
            if chaine2 in liste[k]:
                kw.append(liste[k][8:-4])
            else :
                kw.append(liste[k][8:])
                k+=1
                while chaine2 not in liste[k]:
                    kw.append(liste[k])
                    k+=1
                kw.append(liste[k][:-4])
    if len(kw) == 0 :
        return ""
    kwt=""
    for e in kw:
        kwt+=e+" "
    return kwt
def abstract(id):
    #Renvoit le contenu des abstracts d'un article sous forme d'une chaine de caractères : Sur certains articles, on retrouve des fois plusieurs abstracts ecrits dans des langues differentes
    chaine="""abstract-content">"""
    chaine2 = '</div>'
    data = recuphtml("https://hal.archives-ouvertes.fr/"+id)
    data=data.decode('utf-8')
    liste=data.split()
    ab=[]
    for k in range (len(liste)):
        if chaine in liste[k]:
            k+=1
            while ':' not in liste[k]:
                k+=1
            while chaine2 not in liste[k]:
                ab.append(liste[k])
                k+=1
    if len(ab) == 0 :
        return ""
    abt=""
    for e in ab:
        abt+=e+" "
    return abt

def extractall(links,directory):
    #prend en argument une liste de liens et écrit dans un fichier texte à la suite les donnees des articles presents dans ces lien : id abstracts et keywords
    #ce fichier sera utilisé par la fonction search
    contenu = []
    for link in links:
        ids = recupid(recuphtml(link))
        print(ids)
        for id in ids :
            a=abstract(id)
            k=keywords(id)
            an=normalisation(a).lower()
            kn=normalisation(k).lower()
            contenu.append(str(id)+" "+an+" "+kn)
        f = open(directory,'a')
        for k in contenu:
            f.write(k+"//*//*//")
        f.close()
        contenu = []
        
            
    

def search(requetenonnormalise,directory):
  #prend en argument une chaine correspondant à une requete de recherche
    f = open(directory,'r')
    contenuchaine= f.read()
    contenu= contenuchaine.split("//*//*//")
    #astuce pour séparer les différents données des diff articles
    f.close()
    #normalisation de la requete
    requete = normalisation(requetenonnormalise.lower(), accent=True)
    #on utilise la libraire bm25
    tokenized_contenu = [doc.split(" ") for doc in contenu]
    bm25 = BM25Okapi(tokenized_contenu)
    tokenized_requete =requete.split(" ")
    doc_scores = bm25.get_scores(tokenized_requete)
    best = bm25.get_top_n(tokenized_requete, contenu, n=10)
    #on imprime les resultats
    for element in best :
        try:
            print(element.split()[0])
        except:
            print(element)
            
def creatlinks(linksdebase):
#renvoie pour une liste de liens une liste de liens couvrants les 3 premières pages
    links=[]
    for i in linksdebase:
        for j in range(1,3):
            links.append(i+"&page="+str(j))
        
    return links